{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647dc6d4",
   "metadata": {},
   "source": [
    "# Primer Prompt Engineering\n",
    "\n",
    "\n",
    "**Ref**\n",
    "* [Dair-AI’s Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)  \n",
    "* [Lilian Weng’s blog ](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#instruction-prompting)\n",
    "* [Learn Prompting](https://learnprompting.org/docs/basics/instructions)  \n",
    "* [OpenAI Grad School Math ](https://github.com/openai/grade-school-math)  \n",
    "* [Cohere-Prompt Engineering](https://txt.cohere.com/how-to-train-your-pet-llm-prompt-engineering/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9658a099",
   "metadata": {},
   "source": [
    "## Pormpts\n",
    "\n",
    "Prompt engineering, also known as in-context prompting, is a method for steering an LLM’s behavior towards a particular outcome without updating the model’s weights/parameters. It’s the process of effectively communicating with LLMs to get desired results. Prompt engineering is used on a variety of tasks from question answering to arithmetic reasoning.\n",
    "\n",
    "Prompts are a set of text instructions that LLMs receive to generate a response or complete a task. There are several types of prompts like summarization, inferring or transforming. Thus, Prompt engineering aims to take these prompts and help the model to achieve high accuracy and relevance in its outputs.\n",
    "\n",
    "The two most common types of prompting are zero-shot and few-shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe71b4",
   "metadata": {},
   "source": [
    "### Zero-shot Prompting\n",
    "\n",
    "Zero-shot learning involves feeding the task to LLMs without any examples that indicate the desired output, hence the name zero-shot. For example, one could just feed a model a sentence and expect it to output the sentiment of that sentence.\n",
    "\n",
    "An example below from ChatGPT:\n",
    "\n",
    "> Prompt: \n",
    ">> Classify the text into neutral, negative, or positive.   \n",
    "Text: I think the movie is okay.\n",
    ">\n",
    "> Output: \n",
    ">> Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bb3cd",
   "metadata": {},
   "source": [
    "### Few-shot Prompting\n",
    "\n",
    "Few-shot learning, on the other hand, involves providing the model with a small number of high-quality examples that include both input and desired output for the target task. By seeing these good examples, the model can better understand the user's intention and criteria for generating accurate outputs. As a result, few-shot learning often leads to better performance compared to zero-shot learning. However, this approach can consume more tokens and may encounter context length limitations when dealing with long input and output text.\n",
    "\n",
    "This kind of *in-context learning* using few-shot prompting by offering demonstrations in the prompt can guide the LLM to carry out the task. In other words, conditioning the model on a selection of task-specific examples helps improve the model’s performance.\n",
    "\n",
    "Here is an example:\n",
    "\n",
    "> Prompt:\n",
    ">> A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:  \n",
    "We were traveling in Africa and we saw these very cute whatpus.  \n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    ">\n",
    "> Output:\n",
    ">> When we won the game, we all started to farduddle in celebration.\n",
    "\n",
    "\n",
    "The other example I like:\n",
    "\n",
    "> Prompt:\n",
    ">> Your task is to answer in a consistent style.  \n",
    ">> \\<child\\>: Teach me about patience.  \n",
    ">> \\<grandparent\\>: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n",
    ">>\n",
    ">> \\<child\\>: Teach me about resilience.\n",
    ">\n",
    "> Outputs:\n",
    ">> \\<grandparent\\>: Resilience is like a tree that bends with the wind but never breaks. It is the ability to bounce back from adversity and keep moving forward, even when things get tough. Just like a tree that grows stronger with each storm it weathers, resilience is a quality that can be developed and strengthened over time.\n",
    "    \n",
    "    \n",
    "In both cases, the model was given one example and then was able to generate the answer for the next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fff235",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting \n",
    "\n",
    "Chain-of-Thought (CoT) prompting generates a sequence of short sentences known as **reasoning chains**.\n",
    "These describe step-by-step reasoning logic leading to the final answer with more benefits seen for complex reasoning tasks. \n",
    "\n",
    "![image.png](cot.webp)\n",
    "Ref and image soure:[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ffa9f0",
   "metadata": {},
   "source": [
    "### Few-shot CoT\n",
    "\n",
    "Few-shot CoT allows the model to view a few demonstration of high-quality reasoning chains. Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought prompting enables complex reasoning capabilities through intermediate reasoning steps. One can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
    "\n",
    "For example:\n",
    "> Prompt\n",
    ">> Question: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?  \n",
    "Answer: *It takes Tom $30*4 = 120$ minutes to climb the hill.*  \n",
    "*It takes Tom $120/60 = 2$ hours to climb the hill.*  \n",
    "So the answer is 2.\n",
    ">> \n",
    ">> Question: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost £9.50, and the shoes cost £92. Jack has £40. How much more money does Jack need?  \n",
    "Answer: *The total cost of two pairs of socks is $9.50*2 = 19$.*  \n",
    "*The total cost of the socks and the shoes is $19+92 = 111$.*  \n",
    "*Jack need $111-40 = 71$ more.*  \n",
    "So the answer is 71.  \n",
    ">>\n",
    ">> Question: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3539e",
   "metadata": {},
   "source": [
    "### Zero-shot CoT\n",
    "Zero-shot CoT can be achieved by simplely appending “Let’s think step by step” to the prompt.\n",
    "\n",
    "> Prompt:\n",
    ">> I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?  \n",
    ">> Let's think step by step.\n",
    ">\n",
    "> Output:\n",
    ">> First, you started with 10 apples.  \n",
    "You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.  \n",
    "Then you bought 5 more apples, so now you had 11 apples.  \n",
    "Finally, you ate 1 apple, so you would remain with 10 apples.  \n",
    "\n",
    "Amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92e3d85",
   "metadata": {},
   "source": [
    "\n",
    "![image.png](zero-cot.webp)\n",
    "\n",
    "Ref and image soure: [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86a1f2",
   "metadata": {},
   "source": [
    "## Instruction Prompting and Tuning\n",
    "\n",
    "Instruction prompting is by far the most common usecase of LLMs, especially chatbots such as ChatGPT. As an example of instruction prompting:\n",
    "\n",
    "> Prompt: Define Onomatopoeia in one sentence.\n",
    ">\n",
    "> Output: Onomatopoeia is the use of words that imitate or suggest the natural sound of a thing or action.\n",
    "\n",
    "Instruction tuning seeks to offer instruction prompt examples to the LLM so it can close the train-test discrepancy, where the model was trained on web-scale corpora and tested mostly on instructions, and mimic the real-world usage scenario of chatbots. Stanford’s Alpaca is a recent example that uses instruction tuning to offer performance similar to OpenAI’s GPT3.5 but without performing RLHF.\n",
    "\n",
    "Instruction tuning finetunes a pretrained model with tuples of (task instruction, input, ground truth output) to enables the model to be better aligned to user intention and follow instructions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502f180f",
   "metadata": {},
   "source": [
    "## Recursive Prompting\n",
    "\n",
    "Recursive prompting refers to a method of problem-solving that involves breaking down a complex problem into smaller, more manageable sub-problems, which are then solved recursively through a series of prompts.\n",
    "This approach can be particularly useful for tasks that require compositional generalization, where a language model must learn to combine different pieces of information to solve a problem.\n",
    "\n",
    "In the context of natural language processing, recursive prompting can involve using a few-shot prompting approach to decompose a complex problem into sub-problems, and then sequentially solving the extracted sub-problems using the solution to the previous sub-problems to answer the next one. This approach can be used for tasks such as math problems or question answering, where a language model needs to be able to break down complex problems into smaller, more manageable parts to arrive at a solution.\n",
    "\n",
    "The language model can then solve each sub-problem independently or sequentially, using the solution to the previous sub-problem to answer the next one. For example:\n",
    "\n",
    "Calculate the product of the length and width:\n",
    "prompt: \"What is the product of 8 and 6?\"\n",
    "answer: 48\n",
    "\n",
    "Substitute the given values for length and width into the equation:\n",
    "prompt: \"What is the area of a rectangle with length 8 and width 6?\"\n",
    "answer: \"The area of a rectangle with length 8 and width 6 is 48.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63a998",
   "metadata": {},
   "source": [
    "## Pitfalls of LLMs\n",
    "\n",
    "LLMs are extremely powerful, but they are by no means perfect. There are many pitfalls that we should be aware of when using them.\n",
    "\n",
    "* **Citing Sources**: LLMs for the most part cannot accurately cite sources. This is because they do not have access to the Internet, and do not exactly remember where their information came from. They will frequently generate sources that look good, but are entirely inaccurate.  \n",
    "(Strategies like search augmented LLMs (LLMs that can search the Internet and other sources) can often fix this problem)\n",
    "* **Bias**: LLMs are often biased towards generating stereotypical responses. Even with safe guards in place, they will sometimes say sexist/racist/homophobic things. Be careful when using LLMs in consumer-facing applications, and also be careful when using them in research (they can generate biased results).\n",
    "* **Hallucinations**: LLMs will frequently generate falsehoods when asked a question that they do not know the answer to. Sometimes they will state that they do not know the answer, but much of the time they will confidently give a wrong answer.\n",
    "* **Prompt Hacking**:  Users can often trick LLMs into generating any content they want.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc03d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
