{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ec5863",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An Introduction to Large Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818a351c",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To convert this notebook to slides and show:\n",
    "\n",
    "`jupyter nbconvert LLMs.ipynb --to slides --TemplateExporter.exclude_input=True --post serve`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4045b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Language modelling\n",
    "* The training and using of of ChatGPT on paper\n",
    "* Using LLMs\n",
    "* Prompt engineering\n",
    "* Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910b866",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Language Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df1241",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/whatislm.jpg\" alt=\"What is Language Model\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ed3b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language modelling\n",
    "\n",
    "Language modelling is the task of assigning a probability to sentences in a language. \n",
    "    \n",
    "* Given a sequence of words $(w_1, w_2, w_3, \\ldots ,w_{T})$ of length $T$, a language model assigns a probability \n",
    "    $P(w_{1}, w_{2}, \\ldots ,w_{T})$ to the whole sequence. \n",
    "$\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ P(w_{1}, w_{2}, \\ldots ,w_{T}) = ?\n",
    "$\n",
    "    \n",
    "* This is equivalent to assign a probability for a word following a sequence of words:\n",
    "\n",
    "$\n",
    "\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \n",
    "P(w_1,w_2,\\ldots,w_{T}) = P(w_1,w_2,\\ldots,w_{T-1}) \\times P(w_T|w_1,w_2,\\ldots,w_{T-1})\n",
    "$\n",
    "\n",
    "$\n",
    "\\ \\ \\ \\ \\ \\ \\ \n",
    "\\Rightarrow P(w_T|w_1,w_2,\\ldots,w_{T-1}) = \\dfrac{P(w_1,w_2,\\ldots,w_{T-1})}{P(w_1,w_2,\\ldots,w_{T})}  \n",
    "$\n",
    "\n",
    "* Language model is a probability distribution over sequences of words. \n",
    "* Language modelling is essentially a classification problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7622ecb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Examples of using language models\n",
    "#### speech recognition\n",
    "\n",
    "$\\ \\ \\ \\ A=a_1, a_2, \\ldots, a_m \\ \\ \\ \\ \\ \\ a_i\\in{\\mathcal{A}}$\n",
    "denote a sequence of acoustic symbols from audio signals\n",
    "\n",
    "$\\ \\ \\ \\ W = w_1,w2,\\ldots, w_{n} \\ \\ \\ \\ \\ \\ w_i \\in{\\mathcal{W}}$\n",
    "denote a string of n words, each belonging to a fixed vocabulary $\\mathcal{W}$\n",
    "\n",
    "The speech to text recogniser should decide in favor of a word string $W$ satisfying\n",
    "\n",
    "$\\ \\ \\ \\ \\hat{W} = \\underset{W}{argmax}\\ P(W|A)$\n",
    "\n",
    "where $P(W|A)$ is the probability that the words $W$ were spoken, given the evidence $A$ was observed.\n",
    "\n",
    "Apply Bayes' rule $P(W|A) = \\dfrac{P(W) \\times P(A|W)}{P(A)}$:\n",
    "\n",
    "$\n",
    "\\ \\ \\ \\ \\hat{W} = \\underset{W}{argmax}\\ P(W) \\times P(A|W)\n",
    "$\n",
    "\n",
    "#### Translations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e4315",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Traditional approach to language modelling\n",
    "\n",
    "* Counting in large corpus of text ~ Law of large numbers!\n",
    "\n",
    "* A n-gram is a chunk of n consecutive words.\n",
    "* n-order Markov prpoerty assumption\n",
    "$\n",
    "\\ \\ \\ \\ \\ \\ \\ \n",
    "P(w_T=m|w_1,w_2,\\ldots,w_{T-3}, w_{T-2},w_{T-1}) \\approx P(w_T=m|w_{T-3},w_{T-2},w_{T-1}), 3rd\\ order\\ estimation\\ example\n",
    "$\n",
    "* Collect statistics about how frequent different n-grams are and use these to\n",
    "predict next word.\n",
    "$\n",
    "\\ \\ \\ \\ \\ \\ \\ \n",
    "\\hat{p}(w_T=m|w_{T-3}, w_{T-2},w_{T-1}) =\\dfrac{\\#(w_{T-3},w_{T-2},w_{T-1}, w_{T})}{\\#(w_{T-3},w_{T-2},w_{T-1})}\n",
    "$\n",
    "\n",
    "* Smoothing techniques and other tricks to deal with sparsity problems\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004c2a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Neural Networks for language modelling\n",
    "\n",
    "#### MLP: n-gram of words as input (fixed window) and the probability distribution over the next word as output\n",
    "#### RNNs, LSTM, GRU\n",
    "![Alt text](img/rnn_01.png \"RNN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb53c54d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**RNN Advantages**:\n",
    "* Can process any length input\n",
    "* Computation for step t can (in theory) use information from any steps back\n",
    "* Model size doesn’t increase for longer input context\n",
    "* Same weights applied on every timestep, so there is symmetry in how inputs are processed.\n",
    "\n",
    "**RNN Disadvantages**:\n",
    "* Recurrent computation is slow\n",
    "* In practice, difficult to access information from many steps back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d795a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Seq2seq\n",
    "\n",
    "<img src=\"img/seq_to_seq.png\" alt=\"Sequence to Sequence architecture\" width=\"600\"/>\n",
    "\n",
    "[Ref:Standford CS224N: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260ed3da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Transformer Architecture\n",
    "\n",
    "|Transformer|Simplified Transformer|\n",
    "|:----:|:---:|\n",
    "|<img src=\"img/transformer.png\" alt=\"Simplified Transformer\" width=\"200\"/>|<img src=\"img/transformer_simple.png\" alt=\"Simplified transformer\" width=\"500\"/>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5de7426",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attention\n",
    "* A mechanism to measure how relevant a word in a sentence is to other words\n",
    "* Generate **context vectors** that combines all the relevant words's influence\n",
    "* Use the Query-key-Value to match the word pairs and update the context vector\n",
    "\n",
    "For every word in a sentence, generate a context vector which captures the contextual relationship between that word with other words.\n",
    "\n",
    "### Mulit-head attention\n",
    "* Words have differenct meaning in different context\n",
    "* Words have differenct meaning at different position\n",
    "* Allows one word to focus on multiple other words in a sentence\n",
    "\n",
    "Different attention head captures different semantic aspects of words.\n",
    "\n",
    "### Self attention and cross attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47963e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Single layer of self-attention and feed-forward**\n",
    "\n",
    "<img src=\"img/encoder_with_tensors_2.png\" alt=\"Encoder\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f9f741",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Context vector**\n",
    "\n",
    "<img src=\"img/self-attention-QKV-calculation-a.png\" alt=\"Self attention\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61581623",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Intuition of the attention mechanism\n",
    "\n",
    "**Collaborative filtering** ~ **single head attention**\n",
    "\n",
    "<img src=\"img/collaborative_filtering.webp\" alt=\"Collaborative filtering\" width=\"500\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01b883",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/transformer_QKV.svg\" alt=\"Transformer Query key Value\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9278ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "**Multi head attention** ~ **Context-aware collaborative filtering**\n",
    "<img src=\"img/transformer_multi-headed_self-attention-recap.png\" alt=\"Self attention\" width=\"600\"/>\n",
    "\n",
    "**Context-aware collaborative filtering**: Taking additional contextual information into consideration for similarity measure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7f33c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* More details on training of the RNN and Transformer models will be covered in the future\n",
    "* There are also some previous talks on these topics from [Deep learning Guild](link here) and [NLP Guild](link here)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53fb89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Large Language Models (LLMs)\n",
    "\n",
    "* LLMs are language model consisting of billions parameters\n",
    "* Trained on trillions of tokens\n",
    "* Take advantages of parallel computing based on the transformer architectures\n",
    "* Become general purpose models that excel at a wide range of tasks\n",
    "* Implicitly learned syntax and semantics of human language, the general \"knowledge\" about the world\n",
    "\n",
    "\n",
    "![Alt text](img/TypeOfLLMs.png \"Type of LLMs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e3037",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Training of ChatGPT on paper\n",
    "\n",
    "* Pre-training\n",
    "* Supervised Fine Tuning\n",
    "* Reinforcement Learning from Human Feedback (RLHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d040105b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-training\n",
    "\n",
    "![Alt_text](img/chatGPT_training04.png \"Generative Pretraining\")\n",
    "\n",
    "* Supervised learning on unlabled data\n",
    "* Trained on vast amont of data (1 trillion tokens are equivalent to 15 million books)\n",
    "* We’ll run out of Internet data in the next few years with this this trend of data consuming\n",
    "* Many companies have changed their data terms to prevent others from scraping their data for LLMs\n",
    "* The Internet is being rapidly populated with LLM generated data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb55a28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Shoggoth with a smiley face analogy**\n",
    "* The pretrained model is an untamed monster because it was trained on indiscriminate data scraped from the Internet\n",
    "* This monster was then finetuned on higher quality data\n",
    "* Then the finetuned model was further polished using RLHF to make it customer-appropriate\n",
    "\n",
    "<img src=\"img/shoggoth.jpg\" alt=\"Shoggoth with smiley face\" width=400>\n",
    "\n",
    "\n",
    "**Alignment** - Steering LLMs to intended goals and interests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25460d8d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* The pretrained model is an untamed monster because it was trained on \n",
    "indiscriminate data scraped from the Internet: misinformation, propaganda, conspiracy theories, or attacks against certain demographics.\n",
    "* This monster was then finetuned on higher quality data – \n",
    "StackOverflow, Quora and human annotations – which makes it somewhat socially acceptable.\n",
    "* Then the finetuned model was further polished using RLHF to make it customer-appropriate, finally we get a smiley face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a18293",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Fine Tuning\n",
    "\n",
    "* To optimize the pretrained model to generate useful responses\n",
    "* To show the language model examples of how to appropriately respond\n",
    "* To relieve the user burden of design their own prompts ([GPT3 is a few-shot learner](https://arxiv.org/abs/2005.14165))\n",
    "* OpenAI hire 40 high quality labelers to create around 13,000 (prompt, response) pairs for InstructGPT.\n",
    "* Prompts are designed for different use cases (e.g. question answering, summarization, translation)\n",
    "\n",
    "Example training dataset: [Training language models to follow instructions with human feedback, page 26~33](https://arxiv.org/pdf/2203.02155.pdf)\n",
    "\n",
    "\n",
    "<img src=\"img/ChatGPT_training01.png\" alt=\"SFV\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda0013a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/ChatGPT_SFT.png\" alt=\"Supervised Fine Tunning and RLHF\" width=600>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bab27ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "**The goal of Alignment**\n",
    "\n",
    "\n",
    "Given a particular history, the objective is to maximise the probability \n",
    "the model assigns to the sequence of tokens in the corresponding response.\n",
    "\n",
    "<img src=\"img/imitation_game.jpeg\" alt=\"The goal of Alignment\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2a3a7",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This can be viewed as a typical **imitation learning** setup, or **behaviour cloning**  \n",
    "where we try to mimic an teachers'action distribution conditioned on an input state\n",
    "\n",
    "Teach the model by having it mimic how humans respond in conversations.  \n",
    "The model then creates an **expert policy**, which acts like a rule book for how the model should respond to requests. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e423f28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Reward model\n",
    "\n",
    "**Logistic Regression for Scorecard**\n",
    "\n",
    "<img src=\"img/reward_model.jpeg\" alt=\"Reward model\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603a8cca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Proximal Policy Optimization\n",
    "* Policy\n",
    "* Policy gradient optimization algorithm for updating an existing policy to gain reward\n",
    "\n",
    "<img src=\"img/policy_model.png\" alt=\"Policy model\" width=\"400\"/>\n",
    "\n",
    "[ref: How ChatGPT is Trained](https://www.youtube.com/watch?v=VPRSBzXzavo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38841504",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**PPO**\n",
    "\n",
    "* Policy: A policy, in Reinforcement Learning terminology, is a mapping from action space to state space. It can be imagined to be instructions for the RL agent, in terms of what actions it should take based upon which state of the environment it is currently in.\n",
    "\n",
    "* PPO is a policy gradient optimization algorithm, that is, in each step there is an update to an existing policy to seek improvement on certain parameters It ensures that the update is not too large, that is the old policy is not too different from the new policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fce37d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Performance\n",
    "\n",
    "<img src=\"img/PPO_performance.png\" alt=\"Human evaluations of various models' performance\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c38836",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/chatgpt-training_1.png\" alt=\"Training of chatGPT\" width=700>\n",
    "\n",
    "[Ref: Chip Huyun's Blob](https://huyenchip.com/2023/05/02/rlhf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5a59fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Process of using ChatGPT\n",
    "\n",
    "<img src=\"img/usingChatGPT.png\" alt=\"Using ChatGPT\" width=\"400\"/>\n",
    "\n",
    "**Content Moderation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce8361",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Debatable Topics and Pitfalls of LLMs\n",
    "\n",
    "LLMs are extremely powerful, but it is debatable on what they are actually doing.\n",
    "\n",
    "* **Stochastic parrot?**\n",
    "* **Emergent abilities**\n",
    "* **Hallucinations**\n",
    "* **Bias**:\n",
    "* **Citing Sources on generations**\n",
    "* **Prompt Hacking**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c98c18",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* **Stochastic parrot or not?**: Although large language models are good at generating convincing language, many people believe LLMa do not actually understand the meaning of the language it is processing\n",
    "\n",
    "* **Emergent abilities**: Emergent abilities are skills that suddenly and unpredictably show up (emerge) in AI systems. Theese abilities are not present in smaller models but it seems that there are qualitative changes that come from scaling the AI language models. There are greate debate on how to define and how to measure them.\n",
    "\n",
    "* **Hallucinations**: LLMs will frequently generate falsehoods when asked a question that they do not know the answer to. Sometimes they will state that they do not know the answer, but much of the time they will confidently give a wrong answer.\n",
    "\n",
    "* **Bias**: LLMs are often biased towards generating stereotypical responses. Even with safe guards in place, they will sometimes say sexist/racist/homophobic things. Be careful when using LLMs in consumer-facing applications, and also be careful when using them in research (they can generate biased results).\n",
    "\n",
    "* **Citing Sources**: LLMs for the most part cannot accurately cite sources. This is because they do not have access to the Internet, and do not exactly remember where their information came from. They will frequently generate sources that look good, but are entirely inaccurate.  \n",
    "(Strategies like search augmented LLMs can often fix this problem)\n",
    "\n",
    "* **Prompt Hacking**:  Users can often trick LLMs into generating any content they want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d20434",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Using LLMs \n",
    "\n",
    "### Tasks\n",
    "* Prompting: Generate text or even image directly\n",
    "* Embedding: Extract semantic information from unstructured data for building applicaitons\n",
    "\n",
    "### Applications\n",
    "* Search\n",
    "* Generative\n",
    "* Summarise\n",
    "* Rewrite\n",
    "* Extract\n",
    "* Classify\n",
    "* Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22a2c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Way of using LLMs\n",
    "\n",
    "#### Model-as-a-Service via API\n",
    "\n",
    "* OpenAI API examples:\n",
    "  * Chat: given a conversation, return a response\n",
    "  * Completions: Given a prompt, return multiple predicted completions, with probabilities of alternative tokens at each position\n",
    "  * Edits: given a prompt and an instruction, the model will return an edited version of the prompt\n",
    "  * Image: create, edit, modify images\n",
    "  * Embeddings: get a vector representation of a given input that can be easily consumed by machine learning models and algorithms\n",
    "* Google has similar APIs: PaLM, Imagen, Codey, Chirp, and Embeddings\n",
    "\n",
    "#### Fine-tune an existing third-party Model in a managed environment via API\n",
    "\n",
    "#### Open-source model in a managed environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f6e0b",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Model-as-a-Service via API** \n",
    "\n",
    "The Embeddings API is a powerful tool that can be used to improve the performance of a variety of applications.\n",
    "It can be used for a variety of tasks, such as search, clustering, recommendations, and anomaly detection.\n",
    "\n",
    "**Moderation** API\n",
    "The moderations endpoint can check whether content complies with OpenAI's usage policies. \n",
    "\n",
    "The models classifies the following categories:\n",
    "* hate\n",
    "* hate/threatening\n",
    "* harassment\n",
    "* self-harm, self-harm/intent, self-harm/instructions\n",
    "* sexual sexual services sexual/minors\n",
    "* violence violence/graphic\n",
    "\n",
    "**Fine-tune an existing third-party Model in a managed environment via API**\n",
    "\n",
    "**Advantages for both above approaches**\n",
    "* Low barrier to entry and convenient to implement \n",
    "* Access to the latest, the largest and most sophisticated LLMs\n",
    "\n",
    "**Limitations**\n",
    "* Data residency and privacy\n",
    "* Potentially higher cost\n",
    "* Dependency on third party\n",
    "\n",
    "\n",
    "**Open-source model in a managed environment**\n",
    "\n",
    "**Advantages**\n",
    "* Wide range of choice\n",
    "* Potentially lower cost\n",
    "* Independence\n",
    "\n",
    "**Tradeoffs**\n",
    "* Complexity: Setting up and maintaining a LLM requires data science and engineering expertise. \n",
    "* Smaller scale, and narrower performance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b58593",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Enterprise Architecture\n",
    "\n",
    "**Google**\n",
    "\n",
    "<img src=\"img/googleArchitecture_02.jpg\" alt=\"Google Enterprise Architecture\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a601d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prompt Engineering\n",
    "\n",
    "Steering an LLM’s behavior towards a particular outcome without updating the model’s weights/parameters.\n",
    "\n",
    "\n",
    "### Prompting Design\n",
    "Effectively communicating with LLMs to get desired results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc91e1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Prompts**: The text feed to the model\n",
    "\n",
    "**Prompt engineering**, also known as in-context prompting, is a method for steering an LLM’s behavior towards a particular outcome without updating the model’s weights/parameters. It’s the process of effectively communicating with LLMs to get desired results. Prompt engineering is used on a variety of tasks from question answering to arithmetic reasoning.\n",
    "\n",
    "Prompts are a set of text instructions that LLMs receive to generate a response or complete a task. There are several types of prompts like summarization, inferring or transforming. Thus, Prompt engineering aims to take these prompts and help the model to achieve high accuracy and relevance in its outputs.\n",
    "\n",
    "The two most common types of prompting are zero-shot and few-shot prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4faf45d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zero-shot Prompting\n",
    "\n",
    "An example below from ChatGPT:\n",
    "\n",
    "> Prompt: \n",
    "> Classify the text into neutral, negative, or positive.   \n",
    "Text: I think the movie is okay.\n",
    ">\n",
    "> Output: \n",
    "> Neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4684fd",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Zero-shot Prompting**\n",
    "\n",
    "Zero-shot learning involves feeding the task to LLMs without any examples that indicate the desired output, hence the name zero-shot. For example, one could just feed a model a sentence and expect it to output the sentiment of that sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebe9390",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Few-shot Prompting\n",
    "\n",
    "Providing the model with a small number of high-quality examples that include both input and desired output for the target task. \n",
    "\n",
    "> Prompt:  \n",
    "> A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses the word whatpu is:  We were traveling in Africa and we saw these very cute whatpus.  \n",
    "> To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses the word farduddle is:\n",
    ">\n",
    "> Output:  \n",
    "> When we won the game, we all started to farduddle in celebration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb01cc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Few-shot Prompting**\n",
    "\n",
    "Few-shot learning, on the other hand, involves providing the model with a small number of high-quality examples that include both input and desired output for the target task. By seeing these good examples, the model can better understand the user's intention and criteria for generating accurate outputs. As a result, few-shot learning often leads to better performance compared to zero-shot learning. However, this approach can consume more tokens and may encounter context length limitations when dealing with long input and output text.\n",
    "\n",
    "This kind of *in-context learning* using few-shot prompting by offering demonstrations in the prompt can guide the LLM to carry out the task. In other words, conditioning the model on a selection of task-specific examples helps improve the model’s performance.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a20767",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> Prompt:  \n",
    "> Your task is to answer in a consistent style.  \n",
    "> \\<child\\>: Teach me about patience.  \n",
    "> \\<grandparent\\>: The river that carves the deepest valley flows from a modest spring; the grandest symphony originates from a single note; the most intricate tapestry begins with a solitary thread.\n",
    ">\n",
    "> \\<child\\>: Teach me about resilience.\n",
    ">\n",
    "> Outputs:  \n",
    "> \\<grandparent\\>: Resilience is like a tree that bends with the wind but never breaks. It is the ability to bounce back from adversity and keep moving forward, even when things get tough. Just like a tree that grows stronger with each storm it weathers, resilience is a quality that can be developed and strengthened over time.\n",
    "\n",
    "    \n",
    "In both cases, the model was given one example and then was able to generate the answer for the next.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f48581",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chain-of-Thought Prompting \n",
    "\n",
    "Chain-of-Thought (CoT) prompting generates a sequence of short sentences known as **reasoning chains**.\n",
    "These describe step-by-step reasoning logic leading to the final answer with more benefits seen for complex reasoning tasks. \n",
    "\n",
    "<img src=\"img/cot.webp\" alt=\"CoT\" width=\"800\"/>\n",
    "\n",
    "Ref and image soure:[Wei et al. (2022)](https://arxiv.org/abs/2201.11903)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf11ec0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Few-shot CoT\n",
    "\n",
    "> Prompt  \n",
    "> **Question**: Tom and Elizabeth have a competition to climb a hill. Elizabeth takes 30 minutes to climb the hill. Tom takes four times as long as Elizabeth does to climb the hill. How many hours does it take Tom to climb up the hill?  \n",
    "> **Answer**: It takes Tom $30*4 = 120$ minutes to climb the hill.  \n",
    "> It takes Tom $120/60 = 2$ hours to climb the hill.  \n",
    "> So the answer is 2.  \n",
    "> \n",
    "> **Question**: Jack is a soccer player. He needs to buy two pairs of socks and a pair of soccer shoes. Each pair of socks cost £9.50, and the shoes cost £92. Jack has £40. How much more money does Jack need?  \n",
    "> **Answer**: The total cost of two pairs of socks is $9.50*2 = 19$.  \n",
    "> The total cost of the socks and the shoes is $19+92 = 111$.  \n",
    "> Jack need $111-40 = 71$ more.  \n",
    "> So the answer is 71.  \n",
    ">\n",
    "> **Question**: Marty has 100 centimeters of ribbon that he must cut into 4 equal parts. Each of the cut parts must be divided into 5 equal parts. How long will each final cut be?  \n",
    "> **Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd17a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Zero-shot CoT\n",
    "Zero-shot CoT can be achieved by simplely appending **“Let’s think step by step”** to the prompt.\n",
    "\n",
    "> Prompt:  \n",
    "> I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?  \n",
    "> **Let's think step by step.**  \n",
    ">  \n",
    "> Output:  \n",
    "> First, you started with 10 apples.  \n",
    "> You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.  \n",
    "> Then you bought 5 more apples, so now you had 11 apples.  \n",
    "> Finally, you ate 1 apple, so you would remain with 10 apples. \n",
    "\n",
    "Amazing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359d73e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In summary:\n",
    "\n",
    "![prompt summary](img/zero-cot.webp)\n",
    "\n",
    "Ref and image soure: [Kojima et al. (2022)](https://arxiv.org/abs/2205.11916)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d1a6c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Formalising Prompts\n",
    "\n",
    "There are a few parts of a prompt that are quite common:\n",
    "\n",
    "<img src=\"img/PromptParts.png\" alt=\"Formal prompt\" width=150>\n",
    "\n",
    "[Ref and image source: Learning Prompting](https://learnprompting.org/docs/basics/formalizing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97713c92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An example:\n",
    "\n",
    "> Prompt:  \n",
    "> Medical history:  \n",
    "> --- January 1, 2000: Fractured right arm playing basketball. Treated with a cast.  \n",
    "> --- February 15, 2010: Diagnosed with hypertension. Prescribed lisinopril.  \n",
    "> --- September 10, 2015: Developed pneumonia. Treated with antibiotics and recovered fully.  \n",
    "> --- March 1, 2022: Sustained a concussion in a car accident. Admitted to the hospital and monitored for 24 hours.  \n",
    "> \n",
    "> You are a doctor. Read this medical history and predict risks for the patient:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478fd41",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "* A role\n",
    "* An instruction / task\n",
    "* A question\n",
    "* Context\n",
    "* Examples (few shot)\n",
    "\n",
    "Not all of these occur in every prompt, and there is no standard order for them. The following is another example: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbb626",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A variety of fancy prompting ideas\n",
    "\n",
    "* ReAct: Combines **Re**asoning and **Act**ing with LLMs.  \n",
    "    *\"What's the age of the universe?\" -> \"I need to find more information on the universe\" -> \"[search on Wikipedia]\"*\n",
    "* Code as Reasoning: When given a question, try to write code that solves this question. Then send the code to a programmatic runtime to get the result. \n",
    "* Automatic Prompt Design: Automating the generation and selection of prompts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ffae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fine-Tuning\n",
    "\n",
    "* Task specific tuning can make LLMs more suitable for domain problems and more reliable\n",
    "* Further train the model on new data\n",
    "\n",
    "### Fine-tuning the model\n",
    "\n",
    "<img src=\"img/model_tuning.jpg\" alt=\"Prompt tuning\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce75c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameter-Efficient Tuning Methods (PETM)\n",
    "\n",
    "**Prompt Tuning**: Tune a vector that get sent prepended to the input text\n",
    "\n",
    "<img src=\"img/prompt_tuning.jpg\" alt=\"Prompt tuning\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac961d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
